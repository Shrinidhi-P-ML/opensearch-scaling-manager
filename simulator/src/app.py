import os
import shutil
import json
import math
from datetime import datetime, timedelta
from threading import Thread

from flask import Flask, jsonify, Response, request
from flask_sqlalchemy import SQLAlchemy
from sqlalchemy import text, desc
from werkzeug.exceptions import BadRequest

import constants
from config_parser import parse_config, get_source_code_dir
from open_search_simulator import Simulator
from cluster_dynamic import ClusterDynamic
from plotter import plot_data_points
from open_search_simulator import timeit


app = Flask(__name__)
app.config["SQLALCHEMY_DATABASE_URI"] = "sqlite:///datapoints.db"
app.app_context().push()
if os.path.exists("instance"):
    shutil.rmtree("instance")
db = SQLAlchemy(app)

# Database model to store the datapoints
class DataModel(db.Model):
    status = db.Column(db.String(200))
    cpu_usage_percent = db.Column(db.Float, default=0)
    memory_usage_percent = db.Column(db.Float, default=0)
    heap_usage_percent = db.Column(db.Float, default=0)
    shards_count = db.Column(db.Integer, default=0)
    total_nodes_count = db.Column(db.Integer, default=0)
    active_shards_count = db.Column(db.Integer, default=0)
    active_primary_shards = db.Column(db.Integer, default=0)
    initializing_shards_count = db.Column(db.Integer, default=0)
    unassigned_shards_count = db.Column(db.Integer, default=0)
    relocating_shards_count = db.Column(db.Integer, default=0)
    master_eligible_nodes_count = db.Column(db.Integer, default=0)
    active_data_nodes = db.Column(db.Integer, default=0)
    date_created = db.Column(db.DateTime, default=datetime.now(), primary_key=True)
    disk_usage_percent = db.Column(db.Integer, default=0)
    rolled_index_size = db.Column(db.Float, default=0)


def get_provision_status():
    """
    Returns the status of provision
    """
    return is_provisioning


def set_provision_status(provisioning):
    """
    Sets the provision status
    :param provisioning: boolean value that indicates provision status
    """
    global is_provisioning
    is_provisioning = provisioning

def get_accelerate_flag():
    return accelerate_flag

def set_accelerate_flag(is_accelerated):
    global accelerate_flag
    accelerate_flag = is_accelerated

def get_first_data_point_time():
    """
    Function queries the database for the time corresponding to first data point
    generated by the simulator
    :return: Time corresponding to first data point generated by the simulator
    """
    first_data_point_time = (
        DataModel.query.order_by(DataModel.date_created)
        .with_entities(DataModel.date_created)
        .first()
    )
    return first_data_point_time[0]


def cluster_db_object(cluster):
    """
    Create a DataModel instance that can be dumped into db
    :param cluster: cluster object
    :return: data model
    """
    return DataModel(
        cpu_usage_percent=cluster.cpu_usage_percent,
        memory_usage_percent=cluster.memory_usage_percent,
        heap_usage_percent=cluster.heap_usage_percent,
        date_created=cluster.date_time,
        status=cluster.status,
        total_nodes_count=cluster.total_nodes_count,
        active_shards_count=cluster.active_shards,
        active_primary_shards=cluster.active_primary_shards,
        initializing_shards_count=cluster.initializing_shards,
        unassigned_shards_count=cluster.unassigned_shards,
        relocating_shards_count=cluster.relocating_shards,
        master_eligible_nodes_count=cluster.master_eligible_nodes_count,
        active_data_nodes=cluster.active_data_nodes,
        disk_usage_percent=cluster.disk_usage_percent,
        rolled_index_size=cluster.rolled_index_size,
    )


def overwrite_after_node_count_change(cluster_objects,time=None):
    """
    Calculate the resource utilization after node change operation
    and overwrite the saved data points in db after node change time.
    Also create an overlap on the png file to show new data points
    :param cluster_objects: all cluster objects with new node configuration
    :param date_time: date time object to overwrite date time now
    :return: expiry time
    """
    if time == None:
        date_time = datetime.now()
    else:
        date_time = time
    cluster_objects_post_change = []
    for cluster_obj in cluster_objects:
        if cluster_obj.date_time >= date_time:
            cluster_objects_post_change.append(cluster_obj)
            task = cluster_db_object(cluster_obj)
            db.session.merge(task)
    db.session.commit()
    plot_data_points(
        cluster_objects_post_change, skip_data_ingestion=True, skip_search_query=True
    )
    return

@timeit
def reset_load(sim,time=None):
    """
    The fuction resets shard size from end of simulation
    to shard size configured at current time
    """
    sim.cluster.clear_index_size()
    if time==None:
        now = datetime.now()
    else:
        now = time
    time_now_hour = now - timedelta(minutes=datetime.now().minute,seconds=now.second,
                microseconds=now.microsecond)
    current_disk = (
    DataModel.query.order_by(desc(DataModel.date_created)).filter(DataModel.date_created <= now)
    .with_entities(
        DataModel.__getattribute__(DataModel, constants.STAT_REQUEST['DiskUtil'])
    )
    .first()
    )
    current_rolled_size = (
    DataModel.query.order_by(desc(DataModel.date_created)).filter(DataModel.date_created <= now)
    .with_entities(
        DataModel.__getattribute__(DataModel, 'rolled_index_size')
    )
    .first()
    )
    rolled_index_size = current_rolled_size[0]
    distribution_size = (current_disk[0]/100)*sim.cluster.total_disk_size_gb
    distribution_size-=rolled_index_size
    shard_size = distribution_size/sim.cluster.total_shard_count
    distribution_size-= (sim.cluster.replica_shards_per_index * sim.cluster.primary_shards_per_index 
                        * sim.cluster.index_count * (shard_size))
    sim.cluster.rolled_index_size = rolled_index_size
    sim.cluster.indices[sim.cluster.rolled_over_index_id].shards[0].shard_size = rolled_index_size
    sim.cluster.indices[sim.cluster.rolled_over_index_id].index_size = rolled_index_size
    sim.distribute_load((distribution_size/sim.frequency_minutes)*60)
    
    

def get_duration_for_resimulation(time):
    """
    Fetches duration in minutes to resimulate
    """
    app.app_context().push()
    if time==None:
        time_now = datetime.now()
    else:
        time_now = time
    simulation_end_date_time = DataModel.query.order_by(desc(DataModel.date_created)).with_entities(
        DataModel.__getattribute__(DataModel, 'date_created')
        ).first()
    resimulation_time = math.ceil(((simulation_end_date_time[0] - time_now ).total_seconds())/60)
    return int(resimulation_time)+5

def get_simulated_points():
    """
    Returns simulated data rate, search rate and
    total duration in minutes from the first simulation
    """
    data_rate = sim.simulated_data_rates.copy()
    search_rate = sim.simulated_search_rates.copy()
    total_minutes = sim.total_simulation_minutes
    return data_rate,search_rate,total_minutes

@timeit
def add_node_and_rebalance(nodes, time=None):
    """
    Increments node count in cluster object and rebalances shards
    among available nodes. Re-Simulates data after node addition
    and shard rebalance.
    :param nodes: count of node(s) to be added to cluster
    """
    app.app_context().push()
    data_rate,search_rate,total_minutes = get_simulated_points()
    sim = Simulator(
        configs.cluster,
        configs.data_function,
        configs.search_description,
        configs.searches,
        configs.simulation_frequency_minutes,
    )
    sim.simulated_data_rates = data_rate
    sim.simulated_search_rates = search_rate
    sim.total_simulation_minutes = total_minutes
    duration = get_duration_for_resimulation(time)
    if time==None:
        hour = datetime.now().hour
        minutes = str(datetime.now().minute) if datetime.now().minute > 9 else "0" + str(datetime.now().minute)
    else:
        hour = time.hour
        minutes = str(time.now().minute) if time.now().minute > 9 else "0" + str(time.now().minute)
    reset_load(sim,time)
    is_accelerated = get_accelerate_flag()
    sim.cluster.add_nodes(nodes, is_accelerated)
    set_accelerate_flag(False)
    cluster_objects = sim.run(duration, str(hour) + "_" + minutes + "_00",True,time)
    overwrite_after_node_count_change(cluster_objects,time)
    is_provisioning = get_provision_status()
    is_provisioning = False
    set_provision_status(is_provisioning)

@timeit
def rem_node_and_rebalance(nodes,time=None):
    """
    Decrements node count in cluster object and rebalances the shards
    among the available nodes. Re-Simulates the data once the node is
    removed and shards are distributed
    :param nodes: count of node(s) to be removed from cluster
    """
    app.app_context().push()
    data_rate,search_rate,total_minutes = get_simulated_points()
    sim = Simulator(
        configs.cluster,
        configs.data_function,
        configs.search_description,
        configs.searches,
        configs.simulation_frequency_minutes,
    )
    sim.simulated_data_rates = data_rate
    sim.simulated_search_rates = search_rate
    sim.total_simulation_minutes = total_minutes
    duration = get_duration_for_resimulation(time)
    if time==None:
        hour = datetime.now().hour
        minutes = str(datetime.now().minute) if datetime.now().minute > 9 else "0" + str(datetime.now().minute)
    else:
        hour = time.hour
        minutes = str(time.now().minute) if time.now().minute > 9 else "0" + str(time.now().minute)
    reset_load(sim,time)
    is_accelerated = get_accelerate_flag()
    sim.cluster.remove_nodes(nodes, is_accelerated)
    set_accelerate_flag(False)
    # sim.cluster.cluster_disk_size_used = sim.cluster.calculate_cluster_disk_size()
    cluster_objects = sim.run(duration, str(hour) + "_" + minutes + "_00",True,time)
    overwrite_after_node_count_change(cluster_objects,time)
    is_provisioning = get_provision_status()
    is_provisioning = False
    set_provision_status(is_provisioning)


@app.route("/stats/violated")
def violated_count():
    """
    Endpoint fetches the violated count for a requested metric, threshold and duration,
    The metric,duration and threshold will be sent as query parameter.
    :param metric: represents the metric that is being queried.
    :param duration: represents the time period for fetching the average
    :param threshold: represents the limit considered for evaluating violated count
    :return: count of stat exceeding the threshold for a given duration
    """
    args = request.args
    args.to_dict()
    metric = args.get('metric', type=str)
    duration = args.get('duration', type=int)
    threshold = args.get('threshold', type=float)
    time_now_arg = args.get('time_now', type=str)
    if metric == None or duration == None or threshold == None or len(args) > constants.QUERY_ARG_LENGTH_FOUR:
        return Response(json.dumps("Invalid query parameters"), status=400)

    if len(args) == constants.QUERY_ARG_LENGTH_FOUR and time_now_arg == None:
        return Response(json.dumps("Invalid query parameters"), status=400)
    # calculate time to query for data
    if time_now_arg: 
        try:  
            time_now = datetime.strptime(time_now_arg, constants.TIME_FORMAT)
        except:
            return Response(json.dumps("Invalid query parameters"), status=400)
    else:    
        time_now = datetime.now()
    # Convert the minutes to time object to compare and query for required data points
    query_begin_time = time_now - timedelta(minutes=duration)
    first_data_point_time = get_first_data_point_time()
    try:
        # Fetching the count of data points for given duration.
        data_point_count = (
            DataModel.query.order_by(constants.STAT_REQUEST[metric])
            .filter(DataModel.date_created > query_begin_time)
            .filter(DataModel.date_created <= time_now)
            .count()
        )

        # If expected data points are not present then respond with error
        if first_data_point_time > query_begin_time:
            return Response(json.dumps("Not enough Data points"), status=400)

        # Fetches the count of stat_name that exceeds the threshold for given duration
        stats = (
            DataModel.query.order_by(constants.STAT_REQUEST[metric])
            .filter(
                DataModel.__getattribute__(DataModel, constants.STAT_REQUEST[metric])
                > threshold
            )
            .filter(DataModel.date_created > query_begin_time)
            .filter(DataModel.date_created < time_now)
            .count()
        )

        return jsonify({"ViolatedCount": stats})

    except KeyError:
        return Response(f"stat not found - {metric}", status=404)
    except Exception as e:
        return Response(e, status=404)


@app.route("/stats/avg/", methods=['GET'])
def average():
    """
    The endpoint evaluates average of requested stat for a duration
    returns error if sufficient data points are not present.
    The metric and duration will be sent as query parameter.
    :param metric: represents the stat that is being queried.
    :param duration: represents the time period for fetching the average
    :return: average of the provided metric for the decision period.
    """
    args = request.args
    args.to_dict()
    metric = args.get('metric',type=str) 
    duration = args.get('duration',type=int)
    time_now_arg = args.get('time_now', type=str)
    if metric == None or duration == None or len(args) > constants.QUERY_ARG_LENGTH_THREE:
        return Response(json.dumps("Invalid query parameters"), status=400)

    if len(args) > constants.QUERY_ARG_LENGTH_TWO and time_now_arg == None:
        return Response(json.dumps("Invalid query parameters"), status=400)
    # calculate time to query for data

    if time_now_arg:
        try:   
            time_now = datetime.strptime(time_now_arg, constants.TIME_FORMAT)
        except:
            return Response(json.dumps("Invalid query parameters"), status=400)
    else:    
        time_now = datetime.now()

    # Convert the minutes to time object to compare and query for required data points
    query_begin_time = time_now - timedelta(minutes=duration)
    first_data_point_time = get_first_data_point_time()
    stat_list = []
    try:
        # Fetches list of rows that is filter by stat_name and are filtered by decision period
        avg_list = (
            DataModel.query.order_by(constants.STAT_REQUEST[metric])
            .filter(DataModel.date_created > query_begin_time)
            .filter(DataModel.date_created <= time_now)
            .with_entities(text(constants.STAT_REQUEST[metric]))
            .all()
        )
        for avg_value in avg_list:
            stat_list.append(avg_value[0])

        # If expected data points count are not present then respond with error
        if first_data_point_time > query_begin_time:
            return Response(json.dumps("Not enough Data points"), status=400)

        # check if any data points were collected
        if not stat_list:
            return Response(json.dumps("Not enough Data points"), status=400)

        # Average, minimum and maximum value of a stat for a given decision period
        return jsonify(
            {
                "avg": sum(stat_list) / len(stat_list),
                "min": min(stat_list),
                "max": max(stat_list),
            }
        )

    except KeyError:
        return Response(f"stat not found - {metric}", status=404)
    except Exception as e:
        return Response(e, status=404)


@app.route("/stats/current", methods=['GET'])
def current_all():
    """
    The endpoint returns all the stats from the latest poll,
    Returns error if sufficient data points are not present.
    """
    args = request.args
    args.to_dict()
    metric = args.get('metric', type=str)
    time_now_arg = args.get('time_now', type=str)

    if len(args) > constants.QUERY_ARG_LENGTH_TWO:
        return Response(json.dumps("Invalid query parameters"), status=400)

    if len(args) == constants.QUERY_ARG_LENGTH_TWO and (time_now_arg == None or metric == None):
        return Response(json.dumps("Invalid query parameters"), status=400)
    
    if len(args) == constants.QUERY_ARG_LENGTH_ONE and (time_now_arg == None and metric == None):
        return Response(json.dumps("Invalid query parameters"), status=400)

    if time_now_arg:
        try:   
            time_now = datetime.strptime(time_now_arg, constants.TIME_FORMAT)
        except:
            return Response(json.dumps("Invalid query parameters"), status=400)
    else:    
        time_now = datetime.now()

    if metric!= None:
        try:
            if constants.STAT_REQUEST[metric] == constants.CLUSTER_STATE:
                # is_provisioning = get_provision_status()
                if get_provision_status():
                # if Simulator.is_provision_in_progress():
                    return jsonify({"current": constants.CLUSTER_STATE_YELLOW})
            # Fetches the stat_name for the latest poll
            current_stat = (
                DataModel.query.order_by(desc(DataModel.date_created))
                .with_entities(
                    DataModel.__getattribute__(DataModel, constants.STAT_REQUEST[metric])
                ).filter(DataModel.date_created <= time_now)
                .all()
            )

            # If expected data points count are not present then respond with error
            if len(current_stat) == 0:
                return Response(json.dumps("Not enough Data points"), status=400)

            return jsonify({"current": current_stat[0][constants.STAT_REQUEST[metric]]})

        except KeyError:
            return Response(f"stat not found - {metric}", status=404)
        except Exception as e:
            return Response(e, status=404)

    is_provisioning = get_provision_status()
    if is_provisioning:
        return jsonify(cluster_dynamic.__dict__)
    try:
        stat_dict = {}
        for key in constants.STAT_REQUEST_CURRENT:
            value = (
                DataModel.query.order_by(desc(DataModel.date_created))
                .with_entities(
                    DataModel.__getattribute__(
                        DataModel, constants.STAT_REQUEST_CURRENT[key]
                    )
                ).filter(DataModel.date_created <= time_now)
                .all()
            )
            stat_dict[key] = value[0][0]
        return jsonify(stat_dict)

    except Exception as e:
        return Response(str(e), status=404)


@app.route("/provision/addnode", methods=["POST"])
def add_node():
    """
    Endpoint to simulate that a node is being added to the cluster
    Expects request body to specify the number of nodes added
    :return: total number of resultant nodes and duration of cluster state as yellow
    """
    is_provisioning = get_provision_status()
    if is_provisioning:
        return Response(
            json.dumps(
                "Cannot perform requested operation as Provisioning is in progress"
            ),
            status=404,
        )
    is_provisioning = True
    set_provision_status(is_provisioning)

    try:
        nodes = int(request.json["nodes"])
        node_count = sim.cluster.total_nodes_count + nodes
    except BadRequest as err:
        is_provisioning = False
        set_provision_status(is_provisioning)
        return Response(json.dumps("expected key 'nodes'"), status=404)
    try: 
            time_now_arg = request.json['time_now']
            time_now = datetime.strptime(time_now_arg, constants.TIME_FORMAT)
            set_accelerate_flag(True)
    except:
            time_now = None
    add_node_thread = Thread(target=add_node_and_rebalance, args=(nodes,time_now))
    add_node_thread.start()
    return jsonify({"nodes": node_count})


@app.route("/provision/remnode", methods=["POST"])
def remove_node():
    """
    Endpoint to simulate that a node is being removed from the cluster
    Expects request body to specify the number of nodes added
    :return: total number of resultant nodes and duration of cluster state as yellow
    """
    is_provisioning = get_provision_status()
    if is_provisioning:
        return Response(
            json.dumps(
                "Cannot perform requested operation as Provisioning is in progress"
            ),
            status=404,
        )
    is_provisioning = True
    set_provision_status(is_provisioning)

    try:
        nodes = int(request.json["nodes"])
        node_count = sim.cluster.total_nodes_count - nodes
        if sim.cluster.total_nodes_count - nodes < sim.cluster.min_nodes_in_cluster:
            return Response(
                json.dumps(
                    "Cannot remove more node(s), Minimum nodes required: ",
                    sim.cluster.min_nodes_in_cluster,
                ),
                status=404,
            )
    except BadRequest as err:
        is_provisioning = False
        set_provision_status(is_provisioning)
        return Response(json.dumps("expected key 'nodes'"), status=404)
    try: 
            time_now_arg = request.json['time_now']
            time_now = datetime.strptime(time_now_arg, constants.TIME_FORMAT)
            set_accelerate_flag(True)
    except:
            time_now = None
    rem_node_thread = Thread(target=rem_node_and_rebalance, args=(nodes,time_now))
    rem_node_thread.start()
    return jsonify({"nodes": node_count})


@app.route("/all")
def all_data():
    """
    Endpoint to fetch the count of datapoints generated by simulator
    :return: returns the count of total datapoints generated by the simulator
    """
    count = DataModel.query.with_entities(
        DataModel.cpu_usage_percent, DataModel.memory_usage_percent, DataModel.status
    ).count()
    return jsonify(count)


if __name__ == "__main__":
    db.create_all()
    cluster_dynamic = ClusterDynamic()
    # remove any existing provision lock
    is_provisioning = False
    accelerate_flag = False
    # get configs from config yaml
    configs = parse_config(
        os.path.join(get_source_code_dir(), constants.CONFIG_FILE_PATH)
    )
    # create the simulator object
    sim = Simulator(
        configs.cluster,
        configs.data_function,
        configs.search_description,
        configs.searches,
        configs.simulation_frequency_minutes,
    )
    sim.cluster.cluster_dynamic = cluster_dynamic
    # generate the data points from simulator
    days = len(sim.data_ingestion.states)
    cluster_objects = sim.run(24 * 60 * days)
    # save the generated data points to png
    plot_data_points(cluster_objects)
    # save data points inside db
    for cluster_obj in cluster_objects:
        task = cluster_db_object(cluster_obj)
        db.session.add(task)
    db.session.commit()

    # start serving the apis
    app.run(port=constants.APP_PORT)
